{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following sentences:\n",
    "\n",
    "    The agent will book the to the show for the entire family.\n",
    "    But you can generally book tickets online.\n",
    "    When you book tickets online they provide you with a book of stamps\n",
    "    \n",
    "If you could see the above sentences the word book is used in different context. In first two sentences the word book(verb) refers to the meaning 'reserve' while in the second portion of the third sentence book(noun) refers to a physical entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"\"\"The agent will book the to the show for the entire family.\n",
    "But you can generally book tickets online.\n",
    "When you book tickets online they provide you with a book of stamps\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer, RegexpTokenizer\n",
    "\n",
    "# Lesk Module\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part - 1\n",
    "\n",
    "    Use the Lesk Module to find the similar words of the word *book* using the above sentences. Record your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: The agent will book the to the show for the entire family. \n",
      "\n",
      "Synset('script.n.01')\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance \n",
      "--------------------------------------------\n",
      "\n",
      "LINE: But you can generally book tickets online. \n",
      "\n",
      "Synset('script.n.01')\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance \n",
      "--------------------------------------------\n",
      "\n",
      "LINE: When you book tickets online they provide you with a book of stamps \n",
      "\n",
      "Synset('script.n.01')\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance \n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = sent_tokenize(data)\n",
    "\n",
    "for line in token:\n",
    "    print(\"LINE:\", line, \"\\n\")\n",
    "    print(lesk(line, 'book'))\n",
    "    print(lesk(line, 'book').definition(), \"\\n--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part - 2\n",
    "\n",
    "Tag sentences using Brill Tagger.\n",
    "\n",
    "### Brill Tagger\n",
    "\n",
    "The BrillTagger class is a **transformation-based tagger**. The BrillTagger class uses a series\n",
    "of rules to correct the results of an initial tagger. These rules are scored based on how many\n",
    "errors they correct minus the number of new errors they produce.\n",
    "\n",
    "The idea is simple Brill Tagger tries to correct the mistake made by the inital tagger. Brill tagger inputs an initial tagger and the templates which autmatically tells to create new rules based on the Training Set.\n",
    "\n",
    "**Recommended Steps:**\n",
    "\n",
    "1. Initially tag the sentence using POS Tagger. Then observe the POS tags for the word book in different context\n",
    "2. Then create a tagged_sentence using the POS Tagger correcting it with the mistakes it made.\n",
    "3. Now create a Brill Tagger using an initial tagger (POS) and pass templates(rules) to it.\n",
    "4. Train the Brill Tagger using the Tagged Sentence\n",
    "5. Test the Brill Tagger on the following sentences:\n",
    "       > \"I bought this book from Kerala\"\n",
    "       > \"He will book tickets to Kerala\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The agent will book the to the show for the entire family.',\n",
       " 'But you can generally book tickets online.',\n",
       " 'When you book tickets online they provide you with a book of stamps']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...] \n",
      "\n",
      "------------------------------- \n",
      "\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Sentences\n",
    "brown_sents = brown.sents() \n",
    "\n",
    "# TRAINING DATA = Brown Tagged Sentences\n",
    "brown_tagged_sents = brown.tagged_sents()\n",
    "\n",
    "print(brown_sents, '\\n\\n------------------------------- \\n')\n",
    "print(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DT'), ('agent', 'NN'), ('will', 'MD'), ('book', 'NN'), ('the', 'DT'), ('to', 'TO'), ('the', 'DT'), ('show', 'NN'), ('for', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('family', 'NN'), ('.', '.')], [('But', 'CC'), ('you', 'PRP'), ('can', 'MD'), ('generally', 'RB'), ('book', 'NN'), ('tickets', 'NNS'), ('online', 'VBP'), ('.', '.')], [('When', 'WRB'), ('you', 'PRP'), ('book', 'NN'), ('tickets', 'NNS'), ('online', 'VBP'), ('they', 'PRP'), ('provide', 'VBP'), ('you', 'PRP'), ('with', 'IN'), ('a', 'DT'), ('book', 'NN'), ('of', 'IN'), ('stamps', 'NNS')]]\n"
     ]
    }
   ],
   "source": [
    "test_tagged_sents = []\n",
    "\n",
    "\n",
    "# Using POS TAGGER to tag testing data\n",
    "for line in token:\n",
    "    \n",
    "    words = nltk.word_tokenize(line)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    test_tagged_sents.append(tagged)\n",
    "    \n",
    "print(test_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taggers Used:\n",
    "    - Unigram\n",
    "    - N gram(Bigram)\n",
    "    - Brill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing different Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, BrillTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger - Using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('agent', 'NN'), ('will', 'MD'), ('book', 'NN'), ('the', 'AT'), ('to', 'TO'), ('the', 'AT'), ('show', 'VB'), ('for', 'IN'), ('the', 'AT'), ('entire', 'JJ'), ('family', 'NN'), ('.', '.')] \n",
      "\n",
      "[('But', 'CC'), ('you', 'PPSS'), ('can', 'MD'), ('generally', 'RB'), ('book', 'NN'), ('tickets', 'NNS'), ('online', None), ('.', '.')] \n",
      "\n",
      "[('When', 'WRB'), ('you', 'PPSS'), ('book', 'NN'), ('tickets', 'NNS'), ('online', None), ('they', 'PPSS'), ('provide', 'VB'), ('you', 'PPSS'), ('with', 'IN'), ('a', 'AT'), ('book', 'NN'), ('of', 'IN'), ('stamps', 'NNS')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Unigram\n",
    "tagger = UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "for line in token:\n",
    "    words = word_tokenize(line)\n",
    "    unigram_tagged = tagger.tag(words)\n",
    "    \n",
    "    print(unigram_tagged, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N gram (Bigram Tagger) - Using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('agent', 'NN'), ('will', 'MD'), ('book', None), ('the', None), ('to', None), ('the', None), ('show', None), ('for', None), ('the', None), ('entire', None), ('family', None), ('.', None)] \n",
      "\n",
      "\n",
      "[('But', 'CC'), ('you', 'PPSS'), ('can', 'MD'), ('generally', 'RB'), ('book', None), ('tickets', None), ('online', None), ('.', None)] \n",
      "\n",
      "\n",
      "[('When', 'WRB'), ('you', 'PPSS'), ('book', None), ('tickets', None), ('online', None), ('they', None), ('provide', None), ('you', None), ('with', None), ('a', None), ('book', None), ('of', None), ('stamps', None)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Bigram tagger\n",
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "\n",
    "for line in token:\n",
    "    words = nltk.word_tokenize(line)\n",
    "    bigram_tagged = bigram_tagger.tag(words)\n",
    "    \n",
    "    print(bigram_tagged,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brill Tagger - Using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-03f70406da31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'agent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'show'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'entire'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JJ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'family'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbrill_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbrill_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/StackRoute/NLP/lib/python3.6/site-packages/nltk/tag/brill_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_sents, max_rules, min_score, min_acc)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# test corpus to look more like the training corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         test_sents = [list(self._initial_tagger.tag(untag(sent)))\n\u001b[0;32m--> 249\u001b[0;31m                       for sent in train_sents]\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Collect some statistics on the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/StackRoute/NLP/lib/python3.6/site-packages/nltk/tag/brill_trainer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# test corpus to look more like the training corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         test_sents = [list(self._initial_tagger.tag(untag(sent)))\n\u001b[0;32m--> 249\u001b[0;31m                       for sent in train_sents]\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Collect some statistics on the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/StackRoute/NLP/lib/python3.6/site-packages/nltk/tag/util.py\u001b[0m in \u001b[0;36muntag\u001b[0;34m(tagged_sentence)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/StackRoute/NLP/lib/python3.6/site-packages/nltk/tag/util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tag\n",
    "from nltk.tag import brill\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk import BrillTaggerTrainer     \n",
    "\n",
    "unigram_tagger = UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "templates = [brill.Template(brill.Pos([1,1])),\n",
    "    brill.Template(brill.Pos([2,2])),\n",
    "    brill.Template(brill.Pos([1,2])),\n",
    "    brill.Template(brill.Pos([1,3])),\n",
    "    brill.Template(brill.Pos([1,1])),\n",
    "    brill.Template(brill.Pos([2,2])),\n",
    "    brill.Template(brill.Pos([1,2])),\n",
    "    brill.Template(brill.Pos([1,3])),\n",
    "    brill.Template(brill.Word([-1, -1])),\n",
    "    brill.Template(brill.Word([-1, -1]))]\n",
    "                           \n",
    "trainer = BrillTaggerTrainer(initial_tagger=unigram_tagger, templates=templates, trace=3, deterministic=True)\n",
    "\n",
    "train_data = [('The', 'DT'), ('agent', 'NN'), ('will', 'MD'), ('book', 'VB'), ('the', 'DT'), ('to', 'TO'), ('the', 'DT'), ('show', 'NN'), ('for', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('family', 'NN'), ('.', '.')]\n",
    "\n",
    "brill_tagger = trainer.train(train_data, max_rules=10)          \n",
    "\n",
    "brill_tagger.tag(token[0])                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part - 3\n",
    "\n",
    "    Perform Part-1 again but passing the POS tags produced by the Brill Tagger.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

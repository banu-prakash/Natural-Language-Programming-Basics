{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer, RegexpTokenizer\n",
    "\n",
    "# Stemming and Lemmatizing\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords, state_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS TAG LIST\n",
    "\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "20.\tRB  Adverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data = \"Silently run Pranjal man Pathak. I work for Wipro. I like playing with pets. I am a good human!\"\n",
    "\n",
    "# Reading a txt file\n",
    "text = open('Cat_100.txt').read()i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(sample_data)\n",
    "sentence = sent_tokenize(sample_data)\n",
    "\n",
    "# Using Regex in Tokenizer:\n",
    "pattern = RegexpTokenizer(r'\\w+')\n",
    "filtered_words = pattern.tokenize(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting lang = 'english'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for i in words:\n",
    "    if i not in stop_words:\n",
    "        filtered_words.append(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silent\n",
      "run\n",
      "pranjal\n",
      "man\n",
      "pathak\n",
      ".\n",
      "I\n",
      "work\n",
      "for\n",
      "wipro\n",
      ".\n",
      "I\n",
      "like\n",
      "play\n",
      "with\n",
      "pet\n",
      ".\n",
      "I\n",
      "am\n",
      "a\n",
      "good\n",
      "human\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for i in words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Silently run Pranjal man Pathak. I work for Wipro. I like playing with pets. I am a good human!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Silently', 'RB'), ('run', 'VBN'), ('Pranjal', 'NNP'), ('man', 'NN'), ('Pathak', 'NNP'), ('.', '.')] \n",
      "\n",
      "[('I', 'PRP'), ('work', 'VBP'), ('for', 'IN'), ('Wipro', 'NNP'), ('.', '.')] \n",
      "\n",
      "[('I', 'PRP'), ('like', 'VBP'), ('playing', 'VBG'), ('with', 'IN'), ('pets', 'NNS'), ('.', '.')] \n",
      "\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('good', 'JJ'), ('human', 'NN'), ('!', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences -\n",
    "tokens = sent_tokenize(sample_data)\n",
    "\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tag = nltk.pos_tag(words)\n",
    "    \n",
    "    print(tag,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(sample_data)\n",
    "\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tag = nltk.pos_tag(words)\n",
    "\n",
    "    chunkGram = r''' Chunk: { <RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunked = chunkParser.parse(tag)\n",
    "    \n",
    "    #chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(sample_data)\n",
    "\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tag = nltk.pos_tag(words)\n",
    "\n",
    "    chunkGram = r''' Chunk: {<.*>+}\n",
    "                            }<VB.?|IN|DT>+{'''\n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunked = chunkParser.parse(tag)\n",
    "    \n",
    "    #chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(sample_data)\n",
    "\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tag = nltk.pos_tag(words)\n",
    "    \n",
    "    namedEnt = nltk.ne_chunk(tag)\n",
    "    namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "better\n",
      "good\n",
      "running\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "\n",
    "print(lm.lemmatize('cats'))\n",
    "print(lm.lemmatize('better'))\n",
    "print(lm.lemmatize('better', pos='a'))\n",
    "print(lm.lemmatize('running', pos='a'))\n",
    "print(lm.lemmatize('worst', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "sample_corpus = gutenberg.raw('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of Words related to \"Program\"- \n",
      "\n",
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
      "\n",
      "\n",
      "First Word =  plan.n.01\n",
      "\n",
      "Lemma :  [Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
      "Def :  a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Set of words for the word\n",
    "syn_sets = wordnet.synsets('Program')\n",
    "\n",
    "print(\"Set of Words related to \\\"Program\\\"- \\n\")\n",
    "print(syn_sets)\n",
    "\n",
    "# The first word of set\n",
    "print(\"\\n\\nFirst Word = \", syn_sets[0].name())\n",
    "print(\"\\nLemma : \", syn_sets[0].lemmas())\n",
    "print(\"Def : \", syn_sets[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Synonyms and Antynonyms\n",
    "good_set = wordnet.synsets('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good'] \n",
      "\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for s in good_set:\n",
    "    for lem in s.lemmas():\n",
    "        synonyms.append(lem.name())\n",
    "        if lem.antonyms():\n",
    "            antonyms.append(lem.antonyms()[0].name())\n",
    "\n",
    "print(synonyms, \"\\n\")\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity - Method: WUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.6666666666666666\n",
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "w1 = wordnet.synset('Animal.n.01')\n",
    "w2 = wordnet.synset('Dog.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "w1 = wordnet.synset('Animal.n.01')\n",
    "w2 = wordnet.synset('Human.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "w1 = wordnet.synset('Animal.n.01')\n",
    "w2 = wordnet.synset('Tea.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First remove the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "# List of filtered words without Stopwords\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 1), ('good', 1), ('human', 1), ('!', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Using Frequency Distribution feature of NLTK\n",
    "freqDist_words = nltk.FreqDist(filtered_words)\n",
    "\n",
    "# TOP 5 MOST COMMON words\n",
    "top_5 = freqDist_words.most_common(5)\n",
    "\n",
    "print(top_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
